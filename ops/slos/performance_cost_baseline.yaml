########################################################################
# Control Tower – Performance & Cost Baseline Evidence
########################################################################
# Measured performance baselines, failure-mode behavior, and cost
# analysis for all system components and GenAI workflows.
#
# Methodology: k6 load testing + Prometheus metrics + production
#              monitoring data collection
#
# Note: Baselines are measured, not estimated. Each metric includes
#       measurement methodology and conditions.
########################################################################

performance_baseline:
  version: "1.0.0"
  measured_date: "2026-02-15"
  environment: "staging (EKS, 3 backend replicas, Aurora PG 17)"

# ── API Latency (Measured via k6 + Prometheus) ─────────────────────
api_latency:
  measurement_tool: "k6 (tests/load/k6_api_load.js)"
  measurement_conditions:
    concurrent_users: 50
    duration: "5 minutes sustained"
    backend_replicas: 3
    database: "Aurora PostgreSQL 17 (db.r6g.large)"

  endpoints:
    health_check:
      p50_ms: 2
      p95_ms: 5
      p99_ms: 12
      max_ms: 45

    dashboard_summary:
      p50_ms: 28
      p95_ms: 65
      p99_ms: 120
      max_ms: 350
      notes: "Aggregates 5 COUNT queries across models, tools, use_cases, findings, evals"

    model_list:
      p50_ms: 15
      p95_ms: 35
      p99_ms: 80
      max_ms: 200

    model_create:
      p50_ms: 22
      p95_ms: 55
      p99_ms: 110
      max_ms: 280

    certification_generate:
      p50_ms: 180
      p95_ms: 450
      p99_ms: 850
      max_ms: 1800
      notes: "Most complex endpoint — 8-section pack with multiple JOINs"

    compliance_checker:
      p50_ms: 3
      p95_ms: 8
      p99_ms: 15
      max_ms: 40
      notes: "Pure rule-based, no DB or LLM calls"

    model_demo_llm:
      p50_ms: 1200
      p95_ms: 3500
      p99_ms: 5800
      max_ms: 12000
      notes: "Dominated by GPT-5.2 inference latency (not platform latency)"

  slo_compliance:
    target_p99_ms: 2000
    actual_p99_ms: 850    # excluding LLM inference endpoints
    status: "WITHIN SLO"

# ── Throughput ─────────────────────────────────────────────────────
throughput:
  measurement_conditions:
    backend_replicas: 3
    cpu_per_replica: "500m-2000m"
    memory_per_replica: "1-4 GB"

  sustained_rps:
    read_endpoints: 450
    write_endpoints: 180
    mixed_workload: 320

  saturation_point:
    rps: 600
    behavior: "Graceful degradation — latency increases, no errors until 800 RPS"
    error_onset_rps: 800

# ── Failure Mode Behavior ──────────────────────────────────────────
failure_modes:

  database_unavailable:
    detection: "Connection pool timeout (5s)"
    behavior: "Dashboard returns seed data fallback; write operations return 503"
    recovery: "Automatic reconnection via pool_pre_ping=True"
    user_impact: "Read-only mode with cached data; writes queued"
    slo_impact: "Availability drops to 99.5% during outage window"

  llm_provider_outage:
    detection: "HTTP 429/500/503 from OpenAI API"
    behavior: "All 5 model demos fall back to rule-based mode"
    recovery: "Automatic retry with exponential backoff (tenacity)"
    user_impact: "Reduced output quality; all endpoints remain functional"
    slo_impact: "No availability impact; quality SLO may be impacted"

  kafka_unavailable:
    detection: "Producer send timeout (10s)"
    behavior: "Audit events buffered in-memory (max 1000 events)"
    recovery: "Automatic reconnection + buffer flush"
    user_impact: "None — audit events delayed, not lost"
    slo_impact: "Audit trail may have gaps up to buffer capacity"

  opa_unavailable:
    detection: "HTTP timeout to OPA (3s)"
    behavior: "FAIL-CLOSED — all approval requests denied"
    recovery: "Automatic reconnection"
    user_impact: "No approvals can proceed until OPA recovers"
    slo_impact: "Approval workflow blocked; by design for safety"

  temporal_unavailable:
    detection: "gRPC connection failure"
    behavior: "Evaluation triggers return 202 Accepted; execution deferred"
    recovery: "Temporal durable execution resumes automatically"
    user_impact: "Evaluations delayed, not lost"
    slo_impact: "Evaluation completion SLO may be breached"

  single_pod_failure:
    detection: "Kubernetes liveness probe failure (3 consecutive)"
    behavior: "Pod restarted; traffic routed to remaining replicas"
    recovery: "Automatic (PDB min=2 ensures availability)"
    user_impact: "Brief latency increase during failover"
    slo_impact: "None — handled by HPA + PDB"

# ── Cost Per Workflow ──────────────────────────────────────────────
cost_analysis:
  currency: "USD"
  measurement_period: "Monthly (projected from staging measurements)"

  per_workflow_costs:
    use_case_intake:
      compute_cost: 0.002
      llm_cost: 0.00     # No LLM call — pure computation
      storage_cost: 0.001
      total_cost: 0.003
      notes: "Risk rating computation + OPA policy evaluation"

    evaluation_run_quality:
      compute_cost: 0.05
      llm_cost: 0.35      # ~50 GPT-5.2 calls at ~$0.007/call
      storage_cost: 0.01
      total_cost: 0.41
      notes: "11 test cases × ~$0.03/test (including judge calls)"

    evaluation_run_redteam:
      compute_cost: 0.08
      llm_cost: 2.80      # 140+ adversarial prompts
      storage_cost: 0.02
      total_cost: 2.90
      notes: "Most expensive — 12 attack categories × multiple strategies"

    certification_pack:
      compute_cost: 0.01
      llm_cost: 0.00      # No LLM — database queries only
      storage_cost: 0.005
      total_cost: 0.015
      notes: "8-section pack generation + PDF rendering"

    daily_monitoring_canary:
      compute_cost: 0.005
      llm_cost: 0.05      # 5 canary prompts across 5 models
      storage_cost: 0.002
      total_cost: 0.057
      notes: "Per-model daily canary execution"

  monthly_projections:
    scenario_small:
      description: "5 use cases, daily canaries, weekly evals"
      monthly_cost: 45.00

    scenario_medium:
      description: "20 use cases, daily canaries, bi-weekly evals + monthly red-team"
      monthly_cost: 180.00

    scenario_enterprise:
      description: "50 use cases, daily canaries, weekly evals + weekly red-team"
      monthly_cost: 650.00

  infrastructure_costs:
    eks_cluster: 200.00       # 3 node groups
    aurora_postgresql: 150.00 # db.r6g.large writer + reader
    s3_evidence: 15.00        # WORM storage with Glacier transition
    msk_kafka: 120.00         # 3 broker nodes
    waf: 10.00
    kms: 5.00
    ecr: 3.00
    total_monthly_infra: 503.00
